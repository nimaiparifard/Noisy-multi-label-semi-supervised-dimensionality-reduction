{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T04:21:42.044297200Z",
     "start_time": "2024-02-15T04:21:41.901776500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "y_tilda = np.load(\"Y_tilda.npy\")\n",
    "P =  np.load(\"P.npy\")\n",
    "train_labels = np.load(\"train_labels.npy\")\n",
    "train_dataset = np.load(\"train_dataset.npy\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T04:21:42.820646100Z",
     "start_time": "2024-02-15T04:21:42.794875300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "X = train_dataset @ P\n",
    "train = X[:int(0.3 * X.shape[0])]\n",
    "test = X[int(0.3 * X.shape[0]):]\n",
    "train_labels_ = train_labels[:int(0.3 * train_labels.shape[0])]\n",
    "test_labels_ = train_labels[int(0.3 * train_labels.shape[0]):]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T15:20:14.151756900Z",
     "start_time": "2024-02-14T15:20:14.137397500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, make_scorer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T15:18:50.248542700Z",
     "start_time": "2024-02-14T15:18:48.129285200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "knn_params = {'n_neighbors': [5, 10,]}\n",
    "xgb_params = {'max_depth': [3, 5, 7], 'learning_rate': [0.1, 0.01, 0.001]}\n",
    "rf_params = {'n_estimators': [100, 200, 300], 'max_depth': [None, 5, 10]}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T15:20:17.523833600Z",
     "start_time": "2024-02-14T15:20:17.508414400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "best_models = {}\n",
    "for i in range(train_labels.shape[1]):\n",
    "    # Get the labels for the current column\n",
    "    labels = train_labels[:, i]\n",
    "\n",
    "    # Run KNN with grid search\n",
    "    knn = KNeighborsClassifier()\n",
    "    knn_grid = GridSearchCV(knn, knn_params, scoring=make_scorer(f1_score, average='micro'))\n",
    "    knn_grid.fit(train, train_labels_)\n",
    "    best_models[f'KNN_{i}'] = knn_grid.best_estimator_\n",
    "\n",
    "    # Run XGBoost with grid search\n",
    "    xgb = XGBClassifier()\n",
    "    xgb_grid = GridSearchCV(xgb, xgb_params, scoring=make_scorer(f1_score, average='micro'))\n",
    "    xgb_grid.fit(train, train_labels_)\n",
    "    best_models[f'XGBoost_{i}'] = xgb_grid.best_estimator_\n",
    "\n",
    "    # Run Random Forest with grid search\n",
    "    rf = RandomForestClassifier()\n",
    "    rf_grid = GridSearchCV(rf, rf_params, scoring=make_scorer(f1_score, average='micro'))\n",
    "    rf_grid.fit(train, train_labels_)\n",
    "    best_models[f'RandomForest_{i}'] = rf_grid.best_estimator_"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T15:31:17.508655800Z",
     "start_time": "2024-02-14T15:20:49.964291600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is XGBoost_0 with an average F1 score of 0.7277486761864922\n"
     ]
    }
   ],
   "source": [
    "best_model = None\n",
    "best_score = 0.0\n",
    "\n",
    "for model_name, model in best_models.items():\n",
    "    # Make predictions using the model\n",
    "    predictions = model.predict(test)\n",
    "\n",
    "    # Calculate the F1 score for each label separately\n",
    "    scores = []\n",
    "    for i in range(test_labels_.shape[1]):\n",
    "        label_predictions = predictions[:, i]\n",
    "        label_true = test_labels_[:, i]\n",
    "        label_score = f1_score(label_true, label_predictions)\n",
    "        scores.append(label_score)\n",
    "\n",
    "    # Calculate the average F1 score across all labels\n",
    "    score = np.mean(scores)\n",
    "\n",
    "    # Update the best model if the score is higher\n",
    "    if score > best_score:\n",
    "        best_model = model_name\n",
    "        best_score = score\n",
    "\n",
    "print(f\"The best model is {best_model} with an average F1 score of {best_score}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T15:34:07.860485700Z",
     "start_time": "2024-02-14T15:34:02.077478Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming Loss (HL'): 0.7751674107142857\n",
      "Macro F1-score (MaF1): 0.7277486761864922\n",
      "Micro F1 (MiF1): 0.7375757181006969\n",
      "Average Precision (AP): 0.6704206326291327\n",
      "One-error (OE): 0.8888392857142857\n"
     ]
    }
   ],
   "source": [
    "from evaluation.eval import evaluate_metrics\n",
    "hl, maf1, mif1, rl, ap, oe, cov = evaluate_metrics(test_labels_, xgb_grid.predict(test))\n",
    "print(\"Hamming Loss (HL'):\", 1- hl)\n",
    "print(\"Macro F1-score (MaF1):\", maf1)\n",
    "print(\"Micro F1 (MiF1):\", mif1)\n",
    "print(\"Average Precision (AP):\", ap)\n",
    "print(\"One-error (OE):\", oe)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-14T20:59:33.366341100Z",
     "start_time": "2024-02-14T20:59:33.226947500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import CCA\n",
    "\n",
    "# Run PCA\n",
    "pca = PCA(n_components=20)\n",
    "train_dataset_pca = pca.fit_transform(train_dataset)\n",
    "\n",
    "# Run CCA\n",
    "cca = CCA(n_components=4)\n",
    "train_dataset_cca = cca.fit_transform(train_dataset, train_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T04:22:58.642264300Z",
     "start_time": "2024-02-15T04:22:57.400119900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "X = train_dataset_pca\n",
    "train = X[:int(0.3 * X.shape[0])]\n",
    "test = X[int(0.3 * X.shape[0]):]\n",
    "train_labels_ = train_labels[:int(0.3 * train_labels.shape[0])]\n",
    "test_labels_ = train_labels[int(0.3 * train_labels.shape[0]):]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T04:24:20.702154900Z",
     "start_time": "2024-02-15T04:24:20.678141200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is XGBoost_0 with an average F1 score of 0.7319227831352384\n",
      "Hamming Loss (HL'): 0.7798549107142857\n",
      "Macro F1-score (MaF1): 0.7319227831352384\n",
      "Micro F1 (MiF1): 0.7444782693179609\n",
      "Average Precision (AP): 0.6730030609957874\n",
      "One-error (OE): 0.91875\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "knn_params = {'n_neighbors': [5, 10, ]}\n",
    "xgb_params = {'max_depth': [3, 5, 7], 'learning_rate': [0.1, 0.01, 0.001]}\n",
    "rf_params = {'n_estimators': [100, 200, 300], 'max_depth': [None, 5, 10]}\n",
    "best_models = {}\n",
    "for i in range(train_labels.shape[1]):\n",
    "    # Get the labels for the current column\n",
    "    labels = train_labels[:, i]\n",
    "\n",
    "    # Run KNN with grid search\n",
    "    knn = KNeighborsClassifier()\n",
    "    knn_grid = GridSearchCV(knn, knn_params, scoring=make_scorer(f1_score, average='micro'))\n",
    "    knn_grid.fit(train, train_labels_)\n",
    "    best_models[f'KNN_{i}'] = knn_grid.best_estimator_\n",
    "\n",
    "    # Run XGBoost with grid search\n",
    "    xgb = XGBClassifier()\n",
    "    xgb_grid = GridSearchCV(xgb, xgb_params, scoring=make_scorer(f1_score, average='micro'))\n",
    "    xgb_grid.fit(train, train_labels_)\n",
    "    best_models[f'XGBoost_{i}'] = xgb_grid.best_estimator_\n",
    "\n",
    "    # Run Random Forest with grid search\n",
    "    rf = RandomForestClassifier()\n",
    "    rf_grid = GridSearchCV(rf, rf_params, scoring=make_scorer(f1_score, average='micro'))\n",
    "    rf_grid.fit(train, train_labels_)\n",
    "    best_models[f'RandomForest_{i}'] = rf_grid.best_estimator_\n",
    "best_model = None\n",
    "best_score = 0.0\n",
    "\n",
    "for model_name, model in best_models.items():\n",
    "    # Make predictions using the model\n",
    "    predictions = model.predict(test)\n",
    "\n",
    "    # Calculate the F1 score for each label separately\n",
    "    scores = []\n",
    "    for i in range(test_labels_.shape[1]):\n",
    "        label_predictions = predictions[:, i]\n",
    "        label_true = test_labels_[:, i]\n",
    "        label_score = f1_score(label_true, label_predictions)\n",
    "        scores.append(label_score)\n",
    "\n",
    "    # Calculate the average F1 score across all labels\n",
    "    score = np.mean(scores)\n",
    "\n",
    "    # Update the best model if the score is higher\n",
    "    if score > best_score:\n",
    "        best_model = model_name\n",
    "        best_score = score\n",
    "\n",
    "print(f\"The best model is {best_model} with an average F1 score of {best_score}\")\n",
    "from evaluation.eval import evaluate_metrics\n",
    "\n",
    "hl, maf1, mif1, rl, ap, oe, cov = evaluate_metrics(test_labels_, xgb_grid.predict(test))\n",
    "print(\"Hamming Loss (HL'):\", 1 - hl)\n",
    "print(\"Macro F1-score (MaF1):\", maf1)\n",
    "print(\"Micro F1 (MiF1):\", mif1)\n",
    "print(\"Average Precision (AP):\", ap)\n",
    "print(\"One-error (OE):\", oe)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T04:34:56.878133300Z",
     "start_time": "2024-02-15T04:24:23.991565400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "kpca = KernelPCA(n_components=20, kernel='rbf')\n",
    "train_dataset_kpca = kpca.fit_transform(train_dataset)\n",
    "X = train_dataset_kpca\n",
    "train = X[:int(0.3 * X.shape[0])]\n",
    "test = X[int(0.3 * X.shape[0]):]\n",
    "train_labels_ = train_labels[:int(0.3 * train_labels.shape[0])]\n",
    "test_labels_ = train_labels[int(0.3 * train_labels.shape[0]):]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T04:44:13.254627300Z",
     "start_time": "2024-02-15T04:43:50.074598Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is XGBoost_0 with an average F1 score of 0.7237206721858545\n",
      "Hamming Loss (HL'): 0.7727678571428571\n",
      "Macro F1-score (MaF1): 0.7237206721858545\n",
      "Micro F1 (MiF1): 0.7367127893443683\n",
      "Average Precision (AP): 0.6643653647367982\n",
      "One-error (OE): 0.9457589285714286\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "knn_params = {'n_neighbors': [5, 10, ]}\n",
    "xgb_params = {'max_depth': [3, 5, 7], 'learning_rate': [0.1, 0.01, 0.001]}\n",
    "rf_params = {'n_estimators': [100, 200, 300], 'max_depth': [None, 5, 10]}\n",
    "best_models = {}\n",
    "for i in range(train_labels.shape[1]):\n",
    "    # Get the labels for the current column\n",
    "    labels = train_labels[:, i]\n",
    "\n",
    "    # Run KNN with grid search\n",
    "    knn = KNeighborsClassifier()\n",
    "    knn_grid = GridSearchCV(knn, knn_params, scoring=make_scorer(f1_score, average='micro'))\n",
    "    knn_grid.fit(train, train_labels_)\n",
    "    best_models[f'KNN_{i}'] = knn_grid.best_estimator_\n",
    "\n",
    "    # Run XGBoost with grid search\n",
    "    xgb = XGBClassifier()\n",
    "    xgb_grid = GridSearchCV(xgb, xgb_params, scoring=make_scorer(f1_score, average='micro'))\n",
    "    xgb_grid.fit(train, train_labels_)\n",
    "    best_models[f'XGBoost_{i}'] = xgb_grid.best_estimator_\n",
    "\n",
    "    # Run Random Forest with grid search\n",
    "    rf = RandomForestClassifier()\n",
    "    rf_grid = GridSearchCV(rf, rf_params, scoring=make_scorer(f1_score, average='micro'))\n",
    "    rf_grid.fit(train, train_labels_)\n",
    "    best_models[f'RandomForest_{i}'] = rf_grid.best_estimator_\n",
    "best_model = None\n",
    "best_score = 0.0\n",
    "\n",
    "for model_name, model in best_models.items():\n",
    "    # Make predictions using the model\n",
    "    predictions = model.predict(test)\n",
    "\n",
    "    # Calculate the F1 score for each label separately\n",
    "    scores = []\n",
    "    for i in range(test_labels_.shape[1]):\n",
    "        label_predictions = predictions[:, i]\n",
    "        label_true = test_labels_[:, i]\n",
    "        label_score = f1_score(label_true, label_predictions)\n",
    "        scores.append(label_score)\n",
    "\n",
    "    # Calculate the average F1 score across all labels\n",
    "    score = np.mean(scores)\n",
    "\n",
    "    # Update the best model if the score is higher\n",
    "    if score > best_score:\n",
    "        best_model = model_name\n",
    "        best_score = score\n",
    "\n",
    "print(f\"The best model is {best_model} with an average F1 score of {best_score}\")\n",
    "from evaluation.eval import evaluate_metrics\n",
    "\n",
    "hl, maf1, mif1, rl, ap, oe, cov = evaluate_metrics(test_labels_, xgb_grid.predict(test))\n",
    "print(\"Hamming Loss (HL'):\", 1 - hl)\n",
    "print(\"Macro F1-score (MaF1):\", maf1)\n",
    "print(\"Micro F1 (MiF1):\", mif1)\n",
    "print(\"Average Precision (AP):\", ap)\n",
    "print(\"One-error (OE):\", oe)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T04:54:03.948054500Z",
     "start_time": "2024-02-15T04:44:15.912160200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
